<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>01_퍼셉트론 (Perceptron) | A workbench of Seokhwan’s</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="01_퍼셉트론 (Perceptron)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Perceptron이란?" />
<meta property="og:description" content="Perceptron이란?" />
<meta property="og:site_name" content="A workbench of Seokhwan’s" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-03T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="01_퍼셉트론 (Perceptron)" />
<script type="application/ld+json">
{"url":"/deeplearning/2021/02/03/perceptron.html","description":"Perceptron이란?","@type":"BlogPosting","headline":"01_퍼셉트론 (Perceptron)","dateModified":"2021-02-03T00:00:00+09:00","datePublished":"2021-02-03T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"/deeplearning/2021/02/03/perceptron.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="A workbench of Seokhwan's" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">A workbench of Seokhwan&#39;s</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">01_퍼셉트론 (Perceptron)</h1>
    <p class="post-meta"><time class="dt-published" datetime="2021-02-03T00:00:00+09:00" itemprop="datePublished">
        Feb 3, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Seokhwan Kwon</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="perceptron이란">Perceptron이란?</h2>

<p>필자는 ‘Perceptron’이라는 단어를 듣자마자
이렇게 생각했다. Per(완전히) + cep(잡다, 이해하다) + tron(The Realtime Operating System Nucleus)</p>

<p>tron이 좀 어려운데… 네이버 사진을 캡쳐해왔다.
<img src="/assets/images/사전_tron.png" alt="사전_tron" /></p>

<p>즉, 완전히 이해하기위한 것이다.</p>

<h3 id="perceptron의-역할-및-용어">Perceptron의 역할 및 용어</h3>

<p>일반적으로 뇌의 연산구조는 <code class="language-plaintext highlighter-rouge">뉴런 -&gt; 시냅스 -&gt; 뉴런</code>이렇게 구성되어 있다.
그렇다면, 컴퓨터의 연산구조는 <code class="language-plaintext highlighter-rouge">입력값 -&gt; 활성함수 -&gt; 출력값</code>이렇게 구성할 수 있지 않는가?</p>

<p>우리는 코딩을 통해서 함수에 reference value를 주면, return value를 받는것을 알고 있다. 저 위의 구조와 동일하지 않는가? 어렵지 않다.</p>

<p>\(y = ax + b = wx + b\)</p>
<blockquote>
  <p>위 식은 x의 값(input)에 따라서 y의 값(output)이 달라지는 것이다.
쉽게 이해가 가지 않는가?</p>

  <p>그렇다면 딥러닝에서의 ‘wx + b’로 나타낼 수 있는데 이것을 판단함수 또는 활성화 함수(Activation function)이라고 합니다. (ex. 시그모이드)</p>

  <p>이때, w와 b는 각각 무엇을 의미하는지 아시나요?</p>
</blockquote>

<p>w : weight (가중치)
b : bias (편향, 선입견)
y : weighted sum (가중합)</p>

<blockquote>
  <p>위 용어는 매우 중요함으로 잘 기억해두시길 바랍니다.</p>
</blockquote>

<h3 id="perceptron의-위치">Perceptron의 위치</h3>

<p>뇌 : 뉴런 -&gt; 신경망 -&gt; 지능
컴퓨터 : Perceptron -&gt; 인공신경망 -&gt; 인공지능</p>

<h3 id="perceptron의-구조">Perceptron의 구조</h3>

<p><img src="https://miro.medium.com/max/645/0*LJBO8UbtzK_SKMog" alt="Perceptron_Model" /></p>

<p>일반적으로 Perceptron의 구조는 이렇게 생겼다.</p>

<p>우리는 앞서 XOR을 바로 연산하기 어렵다는 것을 배운 적이 있다.</p>

<p><img src="http://ecee.colorado.edu/~ecen4831/lectures/xor2.gif" alt="Perceptron_logic" /></p>

<h3 id="perceptron의-은닉층">Perceptron의 은닉층</h3>

<p>그래서 은닉층이라는 것을 활용하게 되었다.
이것은 수학에서 치환의 개념으로 생각하면 쉽다. (ex. 극좌표 변환)</p>

<p><img src="/assets/images/Multi_Perceptron.jpg" alt="Multi_Perceptron" /></p>

<p>이런식으로 Multi Perceptron을 구성하여 해결할 수 있다.
위 식을 보면 상당히 복잡해 보이는데… 사실 하나하나 알고보면 그리 어렵지 않다.</p>

<p>\(w(1) = \begin{pmatrix}
w_{11} &amp; w_{12} \\
w_{21} &amp; w_{22}
\end{pmatrix} = \begin{pmatrix}
-2 &amp; 2 \\
-2 &amp; 2
\end{pmatrix}\)
\(w(2)= \binom{w_{31}}{w_{32}} = \binom{1}{1}\)
\(b(1)= \binom{b_{1}}{b_{2}} = \binom{3}{-1}\)
\(b(2) = b_{3} =(-1)\)</p>

<p>각각 이렇게 나타낸다면.
풀이방식은 아래와 같다.</p>

<ol>
  <li>
    <p>n<sub>1</sub>, n<sub>2</sub>를 구한다.
\(\binom{n_{1}}{n_{2}} = \begin{pmatrix}
w_{11} &amp; w_{12} \\
w_{21} &amp; w_{22}
\end{pmatrix} \binom{x_{1}}{x_{2}} + \binom{b_{1}}{b_{2}}
= \binom{w_{11}x_{1} + w_{12}x_{2} + b_{1}}{w_{21}x_{1} + w_{22}x_{2} + b_{2}}\)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">1.</code>에서 구한 n<sub>1</sub>, n<sub>2</sub>를 대입하여, y<sub>out</sub>를 구한다.</p>
  </li>
</ol>

\[y_{out}=  \binom{n_{1}}{n_{2}} \binom{w_{31}}{w_{32}} + b_{3} = n_{1}w_{31} + n_{2}w_{32} + b_{3}\]

<p>위 예시를 가지고 한번 풀어보아라.
그러면 일련의 수식이 나오게 되는데 x<sub>1</sub>, x<sub>2</sub>의 각각 값을 대입해보면 XOR의 연산을 할 수 있을 것이다.</p>

<p><img src="/assets/images/Perceptron예제_연산결과.jpeg" alt="Perceptron예제_연산결과" /></p>

<blockquote>
  <p>위 예제가 바로 Perceptron의 은닉층을 활용하여 XOR를 구하는 기법이다.
여기서 은닉층은 n<sub>1</sub>, n<sub>2</sub>이다.</p>
</blockquote>

<p>그렇다면, w(1), w(2), b(1), b(2)는 어떻게 구해야 하는 것일까?
우리는 이미 주어진 값들을 가지고 풀어보았지만, 실제로는 이 값들을 알아내야 한다.</p>

<p>알아내는 방법은 다음에 배울 “오차 역전파(Back-propagation)”에서 알 수 있다.</p>

  </div><a class="u-url" href="/deeplearning/2021/02/03/perceptron.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Seokhwan Kwon</li>
          <li><a class="u-email" href="mailto:sychar05@gmail.com">sychar05@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Seokhwan-Kwon" title="Seokhwan-Kwon"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
